
========0======
Natural Sciences Tripos, Part IA
Lent Term 2024
Mathematical Methods II, Course B
Prof Natalia Berloﬀ
Section 2
========1======
Natural Sciences Tripos, Part IA · Lent Term 2023
Mathematical Methods II, Course B · Prof Natalia Berloﬀ
2 FUNCTIONS OF MORE THAN ONE VARIABLE
2.1 Introduction
Functions of more than one variable are frequently encountered in scientiﬁc applications when we
think about quantities that vary in more than one direction in space, or that vary in space and time.
The temperature in this room is an example of a scalar ﬁeld . Its value is a real number (when expressed
in some units such as degrees Celsius) that depends on three spatial coordinates x, y and z (and also
time t).
If we wanted to construct a theory of how the temperature evolves in space and time, involving
processes such as thermal conduction, we would ﬁrst need to understand how to do calculus in more
than one variable. How do we deﬁne the temperature gradient when the temperature depends on
three spatial coordinates? How do we ﬁnd the points of minimum and maximum temperature?
These and related questions are answered in this section of the course.
1
========2======
2.2 Partial diﬀerentiation
2.2.1 Ordinary derivative
A function f ( x) of one variable x, where x and f are both real numbers, can be visualized as the curve
y = f ( x) in the ( x,y ) plane, which is known as the graph of the function.
y=f(x)
x x+h
h
f(x+h)-f(x)
f(x)
f(x+h)
y
The (ordinary) derivative of the function is deﬁned by
d f
d x
= f ′( x) = lim
h→0
[
f ( x + h) − f ( x)
h
]
.
Thus d f/ d x is the limiting value of the ratio δf/δx of small increments in the function and its
argument. The function is said to be diﬀerentiable at the point x if this limit exists.
2
========3======
d f/ d x is the local rate of change of the function f ( x) with the variable x. Geometrically, it is the
local gradient of the curve y = f ( x), i.e. the gradient of the straight line that is tangent to the curve
at the point x. The derivative provides the best linear approximation to the function near the point
considered.
2.2.2 Partial derivatives
A function f ( x,y ) of two variables x and y, where x, y and f are all real numbers, can be visualized
as the surface z = f ( x,y ) in the ( x,y,z ) space, which is also known as the graph of the function.
Another way to visualize the function is to plot a selection of contour lines f ( x,y ) = constant in the
( x,y ) plane. This is equivalent to ﬁnding the intersection of the surface z = f ( x,y ) with a selection
of horizontal planes z = constant. (Usually, equally spaced contour values are used.)
3
========4======
-2.5 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 2.5
-2.5
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
2.5
z1
z3z10
The partial derivatives of the function are deﬁned by
∂f
∂x
= fx = lim
h→0
[
f ( x + h,y ) − f ( x,y )
h
]
,
∂f
∂y
= fy = lim
h→0
[
f ( x,y + h) − f ( x,y )
h
]
.
Therefore ∂f/∂x is the rate of change of f with x at constant y, while ∂f/∂y is the rate of change of
f with y at constant x. The fact that two diﬀerent derivatives exist reﬂects the fact that f depends
on two independent variables.
Holding y at a particular constant value means that f ( x,y ) becomes eﬀectively a function of one
variable only. The ordinary derivative of this function is exactly the same as the partial derivative
∂f/∂x .
∂f/∂x is the local gradient of the surface z = f ( x,y ) when travelling in the x-direction, while ∂f/∂y
is the gradient when travelling in the y-direction. These are the gradients of straight lines that are
4
========5======
tangent to the surface and perpendicular to the y- and x-axes, respectively. Together, these two
gradients deﬁne the orientation of the tangent plane , which is the best linear approximation to the
surface near the point considered.
Other notations used for partial derivatives are
∂f
∂x
=
(
∂f
∂x
)
y
= fx = f,x = ∂xf.
We will use the ﬁrst three. The notation
(
∂f
∂x
)
y
or
∂f
∂x
⏐
⏐
⏐
⏐
y
states explicitly which variable (in this case y) is held constant while carrying out the derivative. This
notation is particularly useful when changing independent variables, e.g. from ( x,y ) to ( x,v ), where v
5
========6======
can be expressed as a function of x and y. In such cases there is a very important distinction between(
∂f
∂x
)
y
and
(
∂f
∂x
)
v
(see Section 2.2.8 later).
The use of a subscript to denote diﬀerentiation, as in fx, is compact but potentially ambiguous; we
may want to use a subscript to denote a component of a vector, or for some other purpose.
∂ is sometimes pronounced ‘del’ rather than ‘dee’ to distinguish it from the ordinary diﬀerential
operator d.
A vector can be formed with x and y components equal to fx and fy . This vector is called the gradient
of f and may be written as
grad f = ∇ f =
(
∂f
∂x
,
∂f
∂y
)
= i
∂f
∂x
+ j
∂f
∂y
.
The symbol
∇ =
(
∂
∂x
,
∂
∂y
)
= i
∂
∂x
+ j
∂
∂y
is the gradient operator , a vector diﬀerential operator. It is pronounced ‘grad’, ‘del’ or ‘nabla’. Like
the diﬀerential operator d
d x , it acts on whatever is written to the right of it.
6
========7======
2.2.3 Second derivatives
Second derivatives can be deﬁned by repeated partial diﬀerentiation:
∂2 f
∂x 2 = fxx =
∂
∂x
(
∂f
∂x
)
,
∂2 f
∂y 2 = fyy =
∂
∂y
(
∂f
∂y
)
,
∂2 f
∂y∂x
= fxy =
∂
∂y
(
∂f
∂x
)
,
∂2 f
∂x∂y
= fyx =
∂
∂x
(
∂f
∂y
)
.
While the ﬁrst two of these just mean the second derivative of f with respect to one variable while
the other is held constant, the last two mean something diﬀerent and are known as mixed partial
derivatives .
7
========8======
Example :
f ( x,y ) = e −x2 −y2
∂f
∂x
= fx = −2 x e −x2 −y2
∂f
∂y
= fy = −2 y e −x2 −y2
∂2 f
∂x 2 = fxx =
∂
∂x
(
−2 x e −x2 −y2
)
= ( −2 + 4 x2 ) e −x2 −y2
∂2 f
∂y 2 = fyy =
∂
∂y
(
−2 y e −x2 −y2
)
= ( −2 + 4 y2 ) e −x2 −y2
∂2 f
∂y∂x
= fxy =
∂
∂y
(
−2 x e −x2 −y2
)
= 4 xy e −x2 −y2
∂2 f
∂x∂y
= fyx =
∂
∂x
(
−2 y e −x2 −y2
)
= 4 xy e −x2 −y2
Example :
f ( x,y ) =
1
x + y2
∂f
∂x
= fx = −
1
( x + y2 ) 2
∂f
∂y
= fy = −
2 y
( x + y2 ) 2
8
========9======
∂2 f
∂x 2 = fxx =
∂
∂x
[
−
1
( x + y2 ) 2
]
=
2
( x + y2 ) 3
∂2 f
∂y 2 = fyy =
∂
∂y
[
−
2 y
( x + y2 ) 2
]
= −
2
( x + y2 ) 2 +
8 y2
( x + y2 ) 3
∂2 f
∂y∂x
= fxy =
∂
∂y
[
−
1
( x + y2 ) 2
]
=
4 y
( x + y2 ) 3
∂2 f
∂x∂y
= fyx =
∂
∂x
[
−
2 y
( x + y2 ) 2
]
=
4 y
( x + y2 ) 3
In both examples we observe the symmetry of mixed partial derivatives
∂
∂y
(
∂f
∂x
)
=
∂
∂x
(
∂f
∂y
)
.
This important property holds for all functions (provided that certain smoothness properties are
satisﬁed). We are therefore free to use any of the following notation:
∂2 f
∂x∂y
=
∂2 f
∂y∂x
= fxy = fyx .
9
========10======
To see where this property comes from, let
a = f ( x,y ) , b = f ( x + h,y ) , c = f ( x,y + k) , d = f ( x + h,y + k) .
From the deﬁnition,
∂2 f
∂y∂x
= lim
k→0
1
k
[
lim
h→0
(
d − c
h
)
− lim
h→0
(
b − a
h
)]
while
∂2 f
∂x∂y
= lim
h→0
1
h
[
lim
k→0
(
d − b
k
)
− lim
k→0
(
c − a
k
)]
.
In both cases, therefore, we are interested in the limit of
d − c − b + a
hk
as both h and k tend to zero. For suﬃciently smooth functions f it can be shown that these two
double limits produce the same answer.
10
========11======
2.2.4 Generalization
The deﬁnitions and properties of partial derivatives generalize to functions of any number of variables,
such as f ( x1 ,x 2 ,...,x n ). Thus
∂f
∂x i
= lim
h→0
[
f ( x1 ,x 2 ,...,x i + h,...,x n ) − f ( x1 ,x 2 ,...,x i,...,x n )
h
]
is a partial derivative with respect to xi in which all of the other variables are held constant. If
necessary this can be written
(
∂f
∂x i
)
x1 ,x 2 ,...,x i − 1 ,x i +1 ,...,x n
.
Higher partial derivatives can be calculated in any order, e.g.
∂2 f
∂x i ∂x j
=
∂2 f
∂x j ∂x i
for any i and j, while
∂3 f
∂x i ∂x j ∂x k
=
∂3 f
∂x i ∂x k ∂x j
=
∂3 f
∂x j ∂x i ∂x k
= ··· ,
etc. The partial derivative operators ∂/∂x i and ∂/∂x j commute for any i and j.
11
========12======
2.2.5 Integration
To undo a partial derivative with respect to x at constant y, we integrate with respect to x at constant
y. For example, if f ( x,y ) is known to satisfy
∂f
∂x
= 2 xy 2 ,
then
f = x2 y2 + g( y) ,
where g( y) is a function of y. This arbitrary function replaces the arbitrary constant of integration
that arises when undoing an ordinary derivative. If we are also given, for example, that
∂f
∂y
= 2 x2 y + 2 y,
then we can integrate with respect to y at constant x to ﬁnd
f = x2 y2 + y2 + h( x) ,
where h( x) is a function of x. Comparing the two expressions for f , we see in this case that
g( y) = y2 + h( x) .
This is possible only if h( x) = c = constant, in which case g( y) = y2 + c. Thus f = x2 y2 + y2 + c.
12
========13======
2.2.6 Diﬀerentials
For a diﬀerentiable function of one variable we have a Taylor expansion
f ( x + h) ≈ f ( x) + f ′( x) h +
1
2
f ′′( x) h2 + ··· ,
which gives a local approximation to the variation of f near the point x, as a polynomial in the
displacement h.
(The maximum number of terms that can be included in this expansion depends on how many times
the function is diﬀerentiable. If it is inﬁnitely diﬀerentiable then we can write an inﬁnite series that
converges exactly to f ( x + h) within some interval |h| <L .)
The ﬁrst-order variation, which dominates when h is suﬃciently small, is described by the linear
approximation
f ( x + h) − f ( x) ≈ f ′( x) h, i.e. δf ≈ f ′( x) δx.
Geometrically, this corresponds to approximating the curve y = f ( x) by the tangent line that touches
the curve at the point being considered.
13
========14======
Provided that f ( x) is indeed diﬀerentiable, this approximation is increasingly good as h tends to zero,
in the sense that
lim
h→0
f ( x + h) − f ( x) − f ′( x) h
h
= 0 .
We write the linear approximation as an exact equality of diﬀerentials :
d f = f ′( x) d x.
For example, if f ( x) = x3 , then d f = 3 x2 d x. This statement is exact, whereas the statement
δf ≈ 3 x2 δx is an approximation.
The original (Leibniz’s) interpretation of this type of statement is that it relates an inﬁnitesimal
change d f in f ( x) to an inﬁnitesimal change d x in x. Although both quantities are vanishingly small,
the absolute error in the statement is even smaller, such that the relative error in the statement is
vanishingly small.
Mathematicians have since found more sophisticated ways to formalize this type of statement, but we
will not investigate them here.
14
========15======
Now consider a function f ( x,y ) of two variables. By analogy with the one-variable case, we expect a
local linear approximation to f in the neighbourhood of any point, i.e.
f ( x + h,y + k) ≈ f ( x,y ) + αh + βk,
for some constants α and β. Geometrically, this corresponds to approximating the surface z = f ( x,y )
by the tangent plane that touches the surface at the point being considered.
Let the error in this approximation be
ϵ( h,k ) = f ( x + h,y + k) − f ( x,y ) − αh − βk.
We expect that ϵ( h,k ) vanishes as h and k tend to zero. If
ϵ( h,k )
√
h2 + k2
→ 0 as
√
h2 + k2 → 0
then the function f ( x,y ) is said to be diﬀerentiable at ( x,y ).
To ﬁnd the value of α, take k = 0. Then
ϵ( h, 0)
h
=
f ( x + h,y ) − f ( x,y )
h
− α → 0 as h → 0
and so
α = lim
h→0
[
f ( x + h,y ) − f ( x,y )
h
]
=
∂f
∂x
.
15
========16======
Similarly,
β =
∂f
∂y
.
The linear approximation is therefore
f ( x + h,y + k) − f ( x,y ) ≈
∂f
∂x
h +
∂f
∂y
k,
i.e.
δf ≈
∂f
∂x
δx +
∂f
∂y
δy.
This can also be written as an exact equality between inﬁnitesimals, or diﬀerentials:
d f =
∂f
∂x
d x +
∂f
∂y
d y.
For example, if f = x2 y, then d f = 2 xy d x + x2 d y.
This equation might be interpreted literally as saying that 0 = 0. But it is valuable as a statement
about how the limit is approached and implies more obviously useful results such as
d f
d t
=
∂f
∂x
d x
d t
+
∂f
∂y
d y
d t
,
if x and y are both functions of a parameter t. It is also useful in estimating small ﬁnite changes.
16
========17======
Example . The Clausius–Clapeyron equation
P = P0 exp
[
H
R
(
1
T0
−
1
T
)]
predicts the vapour pressure of a liquid at temperature T if it is known to be P0 at temperature T0 .
H is the enthalpy of vaporization and R is the ideal gas constant.
Suppose that P0 , T0 and R are known exactly. What is the uncertainty in vapour pressure if T is
known only within 5% accuracy and H is known only within 1% accuracy? We regard P as a function
of T and H and calculate the small change in P associated with small changes in T and H ,
d P =
∂P
∂T
d T +
∂P
∂H
d H.
Now
∂P
∂T
=
H
RT 2 P
∂P
∂H
=
1
R
(
1
T0
−
1
T
)
P
d P =
H
RT 2 P d T +
1
R
(
1
T0
−
1
T
)
P d H
d P
P
=
H
RT
d T
T
+
H
R
(
1
T0
−
1
T
)
d H
H
.
17
========18======
This expresses the relation between the fractional errors in P , T and H when those errors are small.
( Exercise : It can be obtained more easily by ﬁnding the partial derivatives of ln P . Note that
d P/P = d ln P .) Thus the fractional error in P is approximately
H
RT
× 0 .05 +
H
R
(
1
T0
−
1
T
)
× 0 .01 .
2.2.7 Taylor series
For a function f ( x) of one variable, diﬀerentiable twice for all x in an interval [ a,b ],
f ( x + h) = f ( x) + f ′( x) h +
1
2
f ′′( x) h2 + R,
where the remainder term is such that
R
h2 → 0 as h → 0 .
The equivalent expansion for a function of two variables is
f ( x + h,y + k) = f ( x,y ) + fx( x,y ) h + fy ( x,y ) k
+
1
2
fxx ( x,y ) h2 + fxy ( x,y ) hk +
1
2
fyy ( x,y ) k2 + R,
where the remainder term is such that
R
h2 + k2 → 0 as h → 0 and k → 0 .
18
========19======
Another way to write this is
f ( x,y ) = f ( x0 ,y 0 ) + fx( x0 ,y 0 )( x − x0 ) + fy ( x0 ,y 0 )( y − y0 )
+
1
2
fxx ( x0 ,y 0 )( x − x0 ) 2 + fxy ( x0 ,y 0 )( x − x0 )( y − y0 )
+
1
2
fyy ( x0 ,y 0 )( y − y0 ) 2 + R.
These series can be extended to higher order if the function is diﬀerentiable more than twice.
19
========20======
2.2.8 Chain rule
Suppose that f is a function of two variables x and y, and that, in turn, x and y are each functions
of two variables u and v. This situation occurs, for example, when we change to a diﬀerent system of
coordinates describing location in the plane.
We could write
f ( x,y ) = f ( x( u,v ) ,y ( u,v )) = g( u,v ) .
From the mathematical point of view f and g are two completely diﬀerent functions. Although their
values are the same, they process their arguments diﬀerently.
For example, if f ( x,y ) = x + y is a linear function of its arguments, but x = u2 and y = v2 , then
g( u,v ) = u2 + v2 is a quadratic function of its arguments. How are the partial derivatives of these two
functions related?
In scientiﬁc applications we usually do not introduce diﬀerent symbols, such as f and g, for the same
quantity when expressed as a function of diﬀerent arguments. One reason for this is that the choice
of symbols is dictated by scientiﬁc conventions. We will call the temperature T , for example, whether
it is expressed as a function of Cartesian coordinates ( x,y ) or polar coordinates ( r,φ ).
20
========21======
We will therefore write
f ( x,y ) = f ( x( u,v ) ,y ( u,v )) = f ( u,v )
although this is (like many things) a conventional abuse of mathematical notation . When doing this
we may have to be careful to indicate (using subscripts) which variable is being held constant in a
partial derivative.
Inﬁnitesimal changes are related by
d x =
(
∂x
∂u
)
v
d u +
(
∂x
∂v
)
u
d v,
d y =
(
∂y
∂u
)
v
d u +
(
∂y
∂v
)
u
d v,
d f =
(
∂f
∂x
)
y
d x +
(
∂f
∂y
)
x
d y.
Substituting the ﬁrst two expressions into the third, we ﬁnd
d f =
[(
∂f
∂x
)
y
(
∂x
∂u
)
v
+
(
∂f
∂y
)
x
(
∂y
∂u
)
v
]
d u
+
[(
∂f
∂x
)
y
(
∂x
∂v
)
u
+
(
∂f
∂y
)
x
(
∂y
∂v
)
u
]
d v.
21
========22======
Since we also expect
d f =
(
∂f
∂u
)
v
d u +
(
∂f
∂v
)
u
d v,
we deduce that
(
∂f
∂u
)
v
=
(
∂f
∂x
)
y
(
∂x
∂u
)
v
+
(
∂f
∂y
)
x
(
∂y
∂u
)
v
,
(
∂f
∂v
)
u
=
(
∂f
∂x
)
y
(
∂x
∂v
)
u
+
(
∂f
∂y
)
x
(
∂y
∂v
)
u
.
These are expressions of the chain rule of partial diﬀerentiation. To remember it, notice the pattern
of the variables.
Example . Change from Cartesian coordinates ( x,y ) to polar coordinates ( r,φ ). By deﬁnition,
x = r cos φ, y = r sin φ.
Thus
(
∂f
∂r
)
φ
=
(
∂f
∂x
)
y
(
∂x
∂r
)
φ
+
(
∂f
∂y
)
x
(
∂y
∂r
)
φ
= cos φ
(
∂f
∂x
)
y
+ sin φ
(
∂f
∂y
)
x
,
22
========23======
(
∂f
∂φ
)
r
=
(
∂f
∂x
)
y
(
∂x
∂φ
)
r
+
(
∂f
∂y
)
x
(
∂y
∂φ
)
r
= −r sin φ
(
∂f
∂x
)
y
+ r cos φ
(
∂f
∂y
)
x
.
Note that we needed to express the old variables ( x,y ) in terms of the new variables ( r,φ ).
To go the other way, we can either solve these simultaneous linear equations to ﬁnd ( exercise )
(
∂f
∂x
)
y
= cos φ
(
∂f
∂r
)
φ
−
sin φ
r
(
∂f
∂φ
)
r
,
(
∂f
∂y
)
x
= sin φ
(
∂f
∂r
)
φ
+
cos φ
r
(
∂f
∂φ
)
r
,
or we can express
r =
√
x2 + y2 , φ = arctan
( y
x
)
,
return to the chain rule and derive the same result ( exercise ).
A special case of the chain rule occurs when only one of the variables is changed. Suppose that u = x,
so we are changing from ( x,y ) to ( x,v ). Then, since ( ∂x/∂x ) v = 1 and ( ∂x/∂v ) x = 0, we have
(
∂f
∂x
)
v
=
(
∂f
∂x
)
y
+
(
∂f
∂y
)
x
(
∂y
∂x
)
v
,
(
∂f
∂v
)
x
=
(
∂f
∂y
)
x
(
∂y
∂v
)
x
.
23
========24======
The ﬁrst equation demonstrates that ( ∂f/∂x ) v is, in general, not equal to ( ∂f/∂x ) y . (This makes
sense because the constraint v = constant is diﬀerent from the constraint y = constant.) The sec-
ond equation corresponds to the chain rule for ordinary derivatives; here x is held constant in each
derivative.
A simpler version of the chain rule occurs if both x and y are functions of a single variable t. This
happens if we restrict attention to a curve in the ( x,y ) plane, parametrized by t. In a similar abuse
of notation, we can write f ( x,y ) = f ( x( t) ,y ( t)) = f ( t). Then
d x =
d x
d t
d t, d y =
d y
d t
d t,
d f =
(
∂f
∂x
)
y
d x +
(
∂f
∂y
)
x
d y
=
[(
∂f
∂x
)
y
d x
d t
+
(
∂f
∂y
)
x
d y
d t
]
d t,
and so
d f
d t
=
(
∂f
∂x
)
y
d x
d t
+
(
∂f
∂y
)
x
d y
d t
.
As noted previously, this can be seen as a direct consequence of the diﬀerential relation
d f =
(
∂f
∂x
)
y
d x +
(
∂f
∂y
)
x
d y.
24
========25======
Example . If the height of a hill is
h( x,y ) =
1
x2 + 2 y4 + 1
and my position as a function of time t is given by x =
√
1 + t and y = 1 − t, what is the rate of
change of my altitude with time?
Use the chain rule in the form
d h
d t
=
(
∂h
∂x
)
y
d x
d t
+
(
∂h
∂y
)
x
d y
d t
.
Now
(
∂h
∂x
)
y
= −
2 x
( x2 + 2 y4 + 1) 2 = −
2
√
1 + t
[1 + t + 2(1 − t) 4 + 1] 2
(
∂h
∂y
)
x
= −
8 y3
( x2 + 2 y4 + 1) 2 = −
8(1 − t) 3
[1 + t + 2(1 − t) 4 + 1] 2
d x
d t
=
1
2
√
1 + t
d y
d t
= −1
and so
d h
d t
=
−1 + 8(1 − t) 3
[2 + t + 2(1 − t) 4 ]2 .
Exercise : check that you get the same answer by substituting x( t) and y( t) into h( x,y ) to ﬁnd h( t)
and then diﬀerentiating with respect to t.
25
========26======
2.2.9 Reciprocity and cyclic relations
If z is a function of two variables x and y, then equally x can be regarded as a function of y and z, or
y can be regarded as a function of z and x.
(We write z rather than f here to emphasize that x, y and z are treated on an equal basis. They
are three quantities sharing a functional relation, which could be written in the form F ( x,y,z ) = 0.
Geometrically, this deﬁnes a surface in three-dimensional space.)
Inﬁnitesimal changes are related by
d x =
(
∂x
∂y
)
z
d y +
(
∂x
∂z
)
y
d z,
d y =
(
∂y
∂z
)
x
d z +
(
∂y
∂x
)
z
d x,
d z =
(
∂z
∂x
)
y
d x +
(
∂z
∂y
)
x
d y.
In each case we can make changes in two variables independently and deduce the change in the third.
Assuming that ( ∂y/∂x ) z ̸= 0, we can rewrite the second equation as
d x =
[
1
/(
∂y
∂x
)
z
]
d y −
[(
∂y
∂z
)
x
/(
∂y
∂x
)
z
]
d z
26
========27======
and compare it with the ﬁrst. Since d y and d z are independent, the coeﬃcients must agree:
(
∂x
∂y
)
z
= 1
/(
∂y
∂x
)
z
, (1)
(
∂x
∂z
)
y
= −
(
∂y
∂z
)
x
/(
∂y
∂x
)
z
. (2)
Equation (1) expresses the reciprocity relation . It is a straightforward generalization of the result
d x
d y = 1
/ d y
d x for ordinary derivatives. It can also be written
(
∂x
∂y
)
z
(
∂y
∂x
)
z
= 1 .
Since x, y and z are on an equal basis, the symbols can be permuted as desired.
Equation (2) expresses the cyclic relation . Using the reciprocity relation, it can also be written in the
more symmetrical form
(
∂x
∂z
)
y
(
∂y
∂x
)
z
(
∂z
∂y
)
x
= −1 .
Note the pattern of the variables and the minus sign.
Example . Given the height of a mountain z = f ( x,y ), what is the slope of the contour lines ( ∂y/∂x ) z ?
Using the cyclic relation,
(
∂y
∂x
)
z
= −
(
∂z
∂x
)
y
/(
∂z
∂y
)
x
= −
(
∂f
∂x
)
y
/(
∂f
∂y
)
x
.
27
========28======
2.2.10 Exact diﬀerentials
The general expression
ω = P ( x,y ) d x + Q ( x,y ) d y
is called a diﬀerential form in the variables x and y.
We say that ω is an exact diﬀerential if there is a function f ( x,y ) such that
P ( x,y ) d x + Q ( x,y ) d y = d f.
Since we know that
d f =
∂f
∂x
d x +
∂f
∂y
d y
and d x and d y are independent, we must then have
∂f
∂x
= P and
∂f
∂y
= Q.
Now the symmetry of mixed partial derivatives
∂
∂y
(
∂f
∂x
)
=
∂
∂x
(
∂f
∂y
)
28
========29======
implies
∂P
∂y
=
∂Q
∂x
.
This is therefore a necessary condition for ω to be an exact diﬀerential. It is also (subject to certain
technical restrictions) a suﬃcient condition. So
P ( x,y ) d x + Q ( x,y ) d y is an exact diﬀerential
if and only if
∂P
∂y
=
∂Q
∂x
.
Example . Is y d x − x d y an exact diﬀerential, i.e. is there a function f such that d f = y d x − x d y?
In this case P = y and Q = −x, so ∂P/∂y = 1 and ∂Q/∂x = −1 are not equal. The answer is no .
Example . Is y d x + x d y an exact diﬀerential?
In this case P = y and Q = x, so ∂P/∂y = 1 = ∂Q/∂x . The answer is yes . To ﬁnd f , we note that
∂f
∂x
= y,
∂f
∂y
= x.
Integrating the ﬁrst equation with respect to x and the second equation with respect to y, we ﬁnd
f = xy + g( y) , f = xy + h( x) .
29
========30======
These equations are compatible only if g( y) = h( x) = constant, in which case f = xy + constant. We
easily check that d f = y d x + x d y as required.
This idea can be applied to ﬁrst-order ordinary diﬀerential equations. Another way of writing the
ODE
d y
d x
= −
P ( x,y )
Q ( x,y )
is
P ( x,y ) d x + Q ( x,y ) d y = 0 .
If P d x+ Q d y is an exact diﬀerential and equal to d f for some function f ( x,y ), then the ODE simpliﬁes
to d f = 0 and its solutions are given by f ( x,y ) = c, where c is an arbitrary constant.
Example . Solve the ODE y d x + x d y = 0.
We have already seen that y d x + x d y = d( xy ) is an exact diﬀerential. Therefore the solutions are
xy = c, i.e. y = c/x .
Compare this solution with the more conventional approach
d y
d x
= −
y
x
∫
d y
y
= −
∫
d x
x
ln y = − ln x + constant
y =
c
x
.
30
========31======
2.2.11 Integrating factors for diﬀerential forms
The function µ( x,y ) is said to be an integrating factor for the diﬀerential form ω = P ( x,y ) d x +
Q ( x,y ) d y if the diﬀerential form
µ( x,y )[ P ( x,y ) d x + Q ( x,y ) d y] is exact.
The condition for this is
∂
∂y
( µP ) =
∂
∂x
( µQ ) .
Given P and Q , it is usually very diﬃcult to solve this (linear partial diﬀerential) equation for µ.
In special cases there might be an integrating factor µ( x) that depends only on x. Then we require
µ
∂P
∂y
=
d µ
d x
Q + µ
∂Q
∂x
,
which can be rearranged to give
1
µ
d µ
d x
=
1
Q
(
∂P
∂y
−
∂Q
∂x
)
.
Given a diﬀerential form P d x + Q d y, if the right-hand side of this equation vanishes, we have an
exact diﬀerential. If the right-hand side is a function of x only, then the equation is self-consistent
31
========32======
and can be integrated to ﬁnd the integrating factor µ( x). If the right-hand side depends on both x
and y, then there is a contradiction and no integrating factor of the form µ( x) exists.
Similarly, an integrating factor of the form µ( y) satisﬁes
1
µ
d µ
d y
= −
1
P
(
∂P
∂y
−
∂Q
∂x
)
and exists if the right-hand side is a function of y only.
Example . ω = y d x − x d y. Here
∂P
∂y
−
∂Q
∂x
= 1 + 1 = 2 ,
so
1
Q
(
∂P
∂y
−
∂Q
∂x
)
= −
2
x
is a function of x only. There is an integrating factor of the form µ( x), with
1
µ
d µ
d x
= −
2
x
ln µ = −2 ln x + constant
µ =
c
x2 .
32
========33======
The simplest choice is µ = 1 /x 2 . Indeed,
µω =
y
x2 d x −
1
x
d y = d
(
−
y
x
)
is an exact diﬀerential.
In fact
−
1
P
(
∂P
∂y
−
∂Q
∂x
)
= −
2
y
is also a function of y only, so µ( y) = 1 /y 2 is also an integrating factor. With this diﬀerent choice,
µω =
1
y
d x −
x
y2 d y = d
(
x
y
)
.
This example shows that integrating factors are not unique. However, if presented with the ODE
y d x − x d y = 0 ,
we could use either version to deduce that the general solution is y = cx , where c is an arbitrary
constant.
33
========34======
2.2.12 Maxwell’s relations
Partial derivatives and diﬀerentials are frequently encountered in thermodynamics, and the classic
application is to gases.
A unit mass of gas has pressure p, volume V and temperature T .
The state of the gas is uniquely deﬁned by any two thermodynamic variables, such as ( p,V ). We say
that the system has two degrees of freedom .
The variables ( p,V,T ) are related by an equation of state . The simplest example is the ideal gas
equation pV = RT , where R is the gas constant. This allows us to express, for example, T as a
function of ( p,V ).
A fourth variable commonly used to describe the state of the gas is the entropy S . In principle, S
can be expressed as a function of ( p,V ). In fact any two of the four variables ( p,V,T,S ) can be used
to describe the state of the gas. The pair ( p,V ) are mechanical variables, while ( T,S ) are thermal
variables.
Physical reasoning leads to the fundamental thermodynamic relation
d U = T d S − p d V, ( ⋆)
where U is the internal energy . This diﬀerential relation is a statement of the ﬁrst law of thermo-
dynamics. In an inﬁnitesimal process, the increase in internal energy is equal to the amount of heat
added to the gas (by adding entropy) plus the amount of work done on it (by compressing it).
34
========35======
Regarding U as a function of ( S,V ), we have
d U =
(
∂U
∂S
)
V
d S +
(
∂U
∂V
)
S
d V.
Therefore
(
∂U
∂S
)
V
= T,
(
∂U
∂V
)
S
= −p.
Symmetry of the mixed partial derivatives implies
∂2 U
∂V ∂S
=
∂2 U
∂S∂V
⇒
(
∂T
∂V
)
S
= −
(
∂p
∂S
)
V
. (1)
This is one of Maxwell’s relations .
We can derive similar relations by making a change of variables known as a Legendre transformation .
Given that
d U = T d S − p d V,
it makes sense to consider the quantity
F = U − TS,
35
========36======
so that
d F = d U − T d S − S d T
= −S d T − p d V.
Regarding F as a function of ( T,V ), we have
d F =
(
∂F
∂T
)
V
d T +
(
∂F
∂V
)
T
d V.
Therefore
(
∂F
∂T
)
V
= −S,
(
∂F
∂V
)
T
= −p,
and so
∂2 F
∂V ∂T
=
∂2 F
∂T∂V
⇒
(
∂S
∂V
)
T
=
(
∂p
∂T
)
V
. (2)
The quantity F is called the Helmholtz free energy or Helmholtz function (some authors prefer the
symbol A). Note, however, that it was just an intermediate step in this calculation. The Maxwell
relation (2) is a purely mathematical consequence of the fundamental thermodynamic relation ( ⋆).
36
========37======
Further Maxwell relations follow from the Legendre transformations H = U + pV ( enthalpy ) and
G = H − TS ( Gibbs free energy or Gibbs function ), which imply
d H = T d S + V d p ⇒
(
∂T
∂p
)
S
=
(
∂V
∂S
)
p
. (3)
d G = −S d T + V d p ⇒
(
∂S
∂p
)
T
= −
(
∂V
∂T
)
p
. (4)
Each Maxwell relation implies the others. For example, if we assume relation (1) then we can deduce
relation (2) as follows:
(
∂S
∂V
)
T
= −
(
∂S
∂T
)
V
(
∂T
∂V
)
S
(cyclic)
= −
(
∂S
∂p
)
V
(
∂p
∂T
)
V
(
∂T
∂V
)
S
(chain)
= −
(
∂p
∂T
)
V
(
∂T
∂V
)
S
/(
∂p
∂S
)
V
(reciprocity)
=
(
∂p
∂T
)
V
(
∂T
∂V
)
S
/(
∂T
∂V
)
S
(Maxwell 1)
=
(
∂p
∂T
)
V
.
Exercise : Choose a diﬀerent pair of Maxwell relations and derive one from the other in a similar way.
37
========38======
A diﬀerent type of relation can also be derived from equation ( ⋆). By considering U as a function of
the two thermal variables ( T,S ) we are led to write
d U = T d S − p d V
= T d S − p
[(
∂V
∂T
)
S
d T +
(
∂V
∂S
)
T
d S
]
= −p
(
∂V
∂T
)
S
d T +
[
T − p
(
∂V
∂S
)
T
]
d S.
Symmetry of the mixed partial derivatives (or, equivalently, the fact that d U is an exact diﬀerential)
implies
(
∂
∂S
)
T
[
−p
(
∂V
∂T
)
S
]
=
(
∂
∂T
)
S
[
T − p
(
∂V
∂S
)
T
]
−
(
∂p
∂S
)
T
(
∂V
∂T
)
S
− p
∂2 V
∂S∂T
= 1 −
(
∂p
∂T
)
S
(
∂V
∂S
)
T
− p
∂2 V
∂T∂S
(
∂p
∂T
)
S
(
∂V
∂S
)
T
−
(
∂p
∂S
)
T
(
∂V
∂T
)
S
= 1 .
38
========39======
2.3 Stationary points
2.3.1 Functions of one variable
Consider a function f ( x) of one variable x, where x and f are both real numbers. Near any point
x = x0 , we can use the Taylor expansion to approximate
f ( x) ≈ f ( x0 ) + f ′( x0 )( x − x0 ) + 1
2 f ′′( x0 )( x − x0 ) 2 ,
provided that the ﬁrst and second derivatives exist. This approximation becomes increasing accurate
as x approaches x0 .
If f ′( x0 ) ̸= 0 then the variation of f ( x) near x = x0 is dominated by the linear term, which has
diﬀerent signs on either side of x = x0 . In this case f ( x) has neither a local minimum nor a local
maximum at x = x0 .
0.5 1.0 1.5 2.0 x
2
4
6
8
10
y
39
========40======
If f ′( x0 ) = 0 then f ( x) is said to have a stationary point at x = x0 , because then it does not vary to
ﬁrst order in x − x0 . Provided that f ′′( x0 ) ̸= 0, the variation of f ( x) near x = x0 is then dominated
by the quadratic term, which has the same sign on either side of x = x0 . In this case f ( x) has
• a local minimum at x = x0 if f ′′( x0 ) > 0
• a local maximum at x = x0 if f ′′( x0 ) < 0
-4 -3 -2 -1 0 1 2 3 4
-2.5
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
2.5
f'(x1)=0
f"(x1)>0
f'(x2)=0
f"(x2)=0
f'(x3)=0
f"(x3)<0
y=f(x)
local minimum
local maximum
x3x2x1
40
========41======
If both f ′( x0 ) = 0 and f ′′( x0 ) = 0 then we would need to inspect higher-order terms in the Taylor
expansion to determine whether the function has a minimum, maximum or neither. For example, if
f ′( x0 ) = f ′′( x0 ) = 0 and f ′′′( x0 ) ̸= 0, then
f ( x) ≈ f ( x0 ) + 1
6 f ′′′( x0 )( x − x0 ) 3 ,
which indicates neither a minimum nor a maximum but rather a stationary point of inﬂection .
(In general, a point of inﬂection is a point on a curve at which the curvature changes sign. The sign
of the curvature of the graph y = f ( x) is the same as the sign of f ′′( x). In fact the curvature is
f ′′(1 + f ′2 ) −3 /2 .)
41
========42======
2.3.2 Functions of two or more variables
Consider now a diﬀerentiable function f ( x,y ) of two variables. Near any point ( x0 ,y 0 ) we have
f ( x,y ) ≈ f ( x0 ,y 0 ) + fx( x0 ,y 0 )( x − x0 ) + fy ( x0 ,y 0 )( y − y0 ) .
The dominant variation is linear in the displacements δx = x − x0 and δy = y − y0 .
If both fx( x0 ,y 0 ) = 0 and fy ( x0 ,y 0 ) = 0 then f ( x,y ) is said to have a stationary point at ( x0 ,y 0 ),
because then it does not vary to ﬁrst order in δx and δy .
In terms of the gradient vector
∇ f =
(
∂f
∂x
,
∂f
∂y
)
= ( fx,f y ) ,
we have
f ( x ) ≈ f ( x 0 ) + [ ∇ f ( x 0 )] ·δx ,
where x 0 = ( x0 ,y 0 ) and δx = ( δx,δy ). The condition for a stationary point is ∇ f ( x 0 ) = 0 . This
result easily generalizes to a function f ( x1 ,x 2 ,...,x n ) of any number of variables.
For an inﬁnitesimal displacement d x = (d x, d y) we can write
d f = ( ∇ f ) ·d x .
At a stationary point where ∇ f = 0 , d f = 0 for all d x .
42
========43======
While a stationary point of a function of one variable is usually a minimum or a maximum (exceptions
occurring only when the second derivative also vanishes), a stationary point of a function of two or
more variable is usually a minimum, a maximum or a saddle point . Near a saddle point, the function
increases in some directions and decreases in others (but to ﬁrst order in the displacement it does not
vary).
(The term extremum refers to either a minimum or a maximum, but not a saddle point.)
Pictures and contour diagrams of an idealised minimum, maximum and saddle point.
43
========44======
Applying a Taylor expansion (see Section 2.2.7) to a function that has a stationary point at ( x0 ,y 0 ),
we obtain the approximation
f ( x,y ) ≈ f ( x0 ,y 0 ) + 1
2 fxx ( x0 ,y 0 )( x − x0 ) 2
+ fxy ( x0 ,y 0 )( x − x0 )( y − y0 ) + 1
2 fyy ( x0 ,y 0 )( y − y0 ) 2 ,
(1)
which is a constant plus a homogeneous quadratic function of δx and δy .
To determine whether f ( x,y ) has a local minimum or maximum, or neither, let us ﬁrst consider the
general homogeneous quadratic function of two variables
Q ( x,y ) = Ax 2 + 2 Bxy + Cy 2 ,
where A, B and C are constants. Note that Q ( x,y ) vanishes at (0 , 0) and has a stationary point there.
Simple examples are:
• (a) x2 + y2 , which has a minimum at (0 , 0)
• (b) −x2 − y2 , which has a maximum at (0 , 0)
• (c) x2 − y2 , which has a saddle point at (0 , 0)
• (d) xy , which has a saddle point at (0 , 0)
44
========45======
More generally, we can complete the square in the form
Q ( x,y ) =
1
A
[
( Ax + By ) 2 + ( AC − B 2 ) y2 ]
,
by noting
Q ( x,y ) =
1
A
[
A2 x2 + 2 ABxy + ACy 2 ]
=
1
A
[
( Ax ) 2 + 2( Ax )( By ) + ( By ) 2 − ( By ) 2 + ACy 2 ]
=
1
A
[
( Ax + By ) 2 + ( AC − B 2 ) y2 ]
which is valid provided that A ̸= 0. Note that ( Ax + By ) and y can be varied independently.
• If A > 0 and AC > B 2 (which requires C > 0) then Q ( x,y ) > 0 for all ( x,y ) ̸= (0 , 0) and we
have a minimum.
• If A < 0 and AC > B 2 (which requires C < 0) then Q ( x,y ) < 0 for all ( x,y ) ̸= (0 , 0) and we
have a maximum.
• If AC <B 2 then Q ( x,y ) can be either positive or negative and we have a saddle point.
45
========46======
Although this analysis fails when A = 0, similar conclusions can be drawn from the alternative way
of completing the square,
Q ( x,y ) =
1
C
[
( Bx + Cy ) 2 + ( AC − B 2 ) x2 ]
,
which is valid provided that C ̸= 0. (If A = C = 0 then Q = 2 Bxy , which has a saddle point, unless
B = 0 also.) We will not discuss all the special cases that are possible if A, C or AC − B 2 vanishes.
Applying this analysis to the approximation (1) for f , we see that
• f has a (local) minimum if fxx fyy >f 2
xy with fxx > 0 and fyy > 0;
• f has a (local) maximum if fxx fyy >f 2
xy with fxx < 0 and fyy < 0;
• f has a saddle point if fxx fyy <f 2
xy .
In special intermediate cases, higher-order terms in the Taylor expansion are needed to determine the
nature of the stationary point. We will not discuss such cases.
46
========47======
For later reference (this will make more sense next term), the Hessian matrix is deﬁned as the matrix
of second derivatives,
H =
(
fxx fxy
fyx fyy
)
.
The symmetry of mixed partial derivatives implies that the Hessian matrix is symmetric. It therefore
has real eigenvalues and orthogonal eigenvectors. A stationary point is a minimum if the eigenvalues
of the Hessian matrix are both positive, a maximum if they are both negative, and a saddle point if
one is negative and the other positive.
▶ To ﬁnd the global minimum and global maximum of a function of two variables may be a more dif-
ﬁcult problem. If the function is bounded and continuous, the global extrema either correspond
to local extrema or occur on the boundary of the domain.
Example . A coal bunker is to be constructed on the side of a house. Assuming that it is a cuboid of
given volume V , ﬁnd the shape that minimizes the external surface area A.
Let the three sides be x, y and z, with x being measured horizontally in a direction perpendicular to
the side of the house. Then V = xyz and A = xy + yz + 2 xz . We can eliminate z by writing z = V/xy
and can then consider
A( x,y ) = xy +
V
x
+
2 V
y
.
47
========48======
To ﬁnd the stationary points of A, set the partial derivatives to zero:
∂A
∂x
= y −
V
x2 = 0
∂A
∂y
= x −
2 V
y2 = 0 .
Eliminate y to ﬁnd x = 2 x4 /V . Since x ̸= 0, we ﬁnd V = 2 x3 and so
x =
(
V
2
)1 /3
, y =
V
x2 = 2 x, z =
V
xy
= x.
The optimal shape is therefore 1 : 2 : 1. To check that A is minimized by this solution, calculate the
second derivatives:
Axx =
2 V
x3 = 4
Ayy =
4 V
y3 = 1
Axy = 1 .
Thus Axx Ayy >A 2
xy with Axx > 0 and Ayy > 0, so A has a local minimum as expected.
(In the next section we will see an alternative method for minimizing A subject to the constraint
V = constant.)
48
========49======
Example . Find and classify the stationary points of f ( x,y ) = x3 −4 x2 +2 xy −y2 . Sketch the contours
of f .
For stationary points,
fx = 3 x2 − 8 x + 2 y = 0
fy = 2 x − 2 y = 0 .
Thus y = x and 3 x2 − 6 x = 3 x( x − 2) = 0. So the stationary points are (0 , 0) and (2 , 2).
The second derivatives are
fxx = 6 x − 8 =
{
−8 at (0,0)
4 at (2,2)
fyy = −2
fxy = 2 .
Therefore (0 , 0) is a maximum because fxx fyy >f 2
xy with fxx < 0 and fyy < 0. But (2 , 2) is a saddle
point because fxx fyy < 0 there.
A way to discover the orientation of the saddle point is to note that the Taylor expansion to second
order near (2 , 2) is
f ≈ c + 2 δx 2 + 2 δxδy − δy 2 ,
49
========50======
where c = f (2 , 2) = −4, δx = x − 2 and δy = y − 2. So the contour lines that pass through the saddle
point are given by f = c, i.e.
2 δx 2 + 2 δxδy − δy 2 ≈ 0
−( δy − δx ) 2 ≈− 3 δx 2
δy ≈ (1 ±
√
3) δx.
-100
-100
-80
-80
-60
-60 -40
-40
-40
-20
-20
-20
0
-10
-10
-10
-10
-8
-8
-8
-8
-6
-6 -6
-6
-4
-4
-4
-4 -2
-2
-2
02
4
6
8
10
-100
-100
-80
-80
-60
-60 -40
-40
-40
-20
-20
-20
0
-10
-10
-10
-10
-8
-8
-8
-8
-6
-6 -6
-6
-4
-4
-4
-4 -2
-2
-2
02
4
6
8
10
-4 -3 -2 -1 0 1 2 3 4
-4
-3
-2
-1
0
1
2
3
4
f=0
S P
Max
50
========51======
2.4 Conditional stationary values
2.4.1 Two independent variables, one constraint
We have seen that the function f ( x,y ) is stationary when ∇ f = 0 , which means that f does not
change to ﬁrst order when either x or y is varied. The stationary points and values found in this way
are called unconditional or unconstrained .
We may be interested in a problem in which x and y cannot be varied independently, but are related
by a condition or constraint of the form g( x,y ) = 0, which means that the points ( x,y ) lie on a curve.
Example . Find the maximum value of f ( x,y ) = xy on the unit circle g( x,y ) = x2 + y2 − 1 = 0.
Method 1 . Solve the constraint equation for y to ﬁnd y = s(1 − x2 ) 1 /2 , where s = ±1, then
substitute to ﬁnd f = sx (1 −x2 ) 1 /2 , which is a (double-valued) function of one variable. The derivative
d f/ d x = s(1 − x2 ) 1 /2 − sx 2 (1 − x2 ) −1 /2 = s(1 − 2 x2 )(1 − x2 ) −1 /2 vanishes at x = ±1 /
√
2, where
f = ±s/ 2. Therefore the maximum of f is 1 /2 and occurs at x = y = 1 /
√
2 and at x = y = −1 /
√
2.
Method 2 . Use a parametric representation x = cos θ, y = sin θ, 0 ⩽ θ< 2 π to satisfy the constraint.
Then f = cos θ sin θ = 1
2 sin 2 θ, which is a function of one variable. The derivative d f/ d θ = cos 2 θ
vanishes at θ = π/ 4, 3 π/ 4, 7 π/ 4 and 11 π/ 4. The ﬁrst and third solutions give the maximum values
f = 1 /2 as above.
Either of these methods is reasonable, but they both rely on the ability to solve the constraint equation.
51
========52======
In more complicated cases this may not be practical, or it may lead to unwieldy algebra. A more
sophisticated approach is the method of Lagrange multipliers .
Method 3 . Suppose the maximum of f on the curve g = 0 is at ( x0 ,y 0 ). Consider the variation of
f ( x,y ) and g( x,y ) near this point. We have
d f = fx d x + fy d y,
d g = gx d x + gy d y,
where we recall that these diﬀerentials express the ﬁrst-order variations. In order to stay on the curve
g = 0, the displacements d x and d y are constrained by
gx d x + gy d y = 0 .
This means that the vector displacement d x = (d x, d y) is tangent to the curve g = 0. Solving this
equation for either d x or d y and substituting into the expression for d f , we ﬁnd
d f = ( fxgy − fy gx)
d x
gy
= −( fxgy − fy gx)
d y
gx
.
(The ﬁrst of these expressions fails when gy = 0 and the second when gx = 0, but in most cases at
least one will work.) f is therefore stationary along the curve when
fxgy − fy gx = 0 ,
52
========53======
or, equivalently,
fx
gx
=
fy
gy
= λ,
where λ is a number to be determined. This is equivalent to
( fx,f y ) = λ( gx,g y ) , i.e. ∇ f = λ∇ g.
Now apply this method to the example. We have
fx = y, f y = x, g x = 2 x, g y = 2 y,
so the equations fx = λg x and fy = λg y imply
y = 2 λx,
x = 2 λy.
We also have the constraint
x2 + y2 = 1 ,
which makes three equations in three unknowns ( x,y,λ ). The ﬁrst two equations imply x = 4 λ2 x, so
either x = 0 or λ = ±1
2 . x = 0 does not work, because then y = 0 and x2 + y2 ̸= 1. If λ = 1
2 then
y = x and we ﬁnd x = y = ±1 /
√
2, so f = 1 /2. If λ = −1
2 then y = −x and we ﬁnd x = −y = ±1 /
√
2,
so f = −1 /2. In this way we ﬁnd all the conditional stationary points; the ﬁrst two are the maxima.
53
========54======
2.4.2 Lagrange multipliers and Lagrangian function
To ﬁnd the stationary points of f ( x,y ) subject to the constraint g( x,y ) = 0, solve the simultaneous
equations
fx = λg x, f y = λg y , g = 0
for x, y and λ, where λ is a Lagrange multiplier .
It can be helpful to deﬁne the Lagrangian function
L( x,y,λ ) = f ( x,y ) − λg ( x,y ) .
Then the method of Lagrange multipliers is equivalent to solving the equations
Lx = Ly = Lλ = 0 ,
i.e. to ﬁnding the unconditional stationary values of the Lagrangian function.
Note:
• It is not really necessary to calculate the derivative Lλ because we already know the form of the
constraint equation g = 0.
• Sometimes the Lagrangian L = f − λg is used to ﬁnd the stationary points of f subject to the
constraint g = c, where c is a non-zero constant. In that case we should solve the equations
Lx = Ly = 0 and g = c (not Lλ = 0).
• Sometimes L = f + λg is used instead of L = f − λg . The result is the same.
54
========55======
2.4.3 Geometrical viewpoint
Consider a path going over a hill, but not visiting the summit. The highest point on the path is where
the contour lines are parallel to the path. At the point the direction of steepest ascent is perpendicular
to the path.
0 0.5 1 1.5
0
0.5
1
1.5
grad f
grad f
grad f
grad g
g(x,y)=0
f(x,y)=const
grad f
At each point, the vector ∇ f is normal to the contour line f = constant passing through that point.
55
========56======
This is because
d f = ( ∇ f ) ·d x ,
which means that the direction in which f does not change (d f = 0) corresponds to a displacement
d x perpendicular to ∇ f .
Similarly, the vector ∇ g is normal to the curve g = 0 at each point on it. When the constraint g = 0
is applied, displacements must be tangent to the curve and therefore satisfy ( ∇ g) ·d x = 0. For the
conditional stationary point, we require that d f = ( ∇ f ) ·d x = 0 not for all d x , but only for those
displacements such that ( ∇ g) ·d x = 0.
Therefore we require that ∇ f is parallel to ∇ g, i.e. ∇ f = λ∇ g.
56
========57======
2.4.4 More than two independent variables, one constraint
A similar method can be applied to functions of more than two variables, as in the following example.
Note that the constraint equation g = 0 conﬁnes us to a surface rather than a curve.
Example . Minimize xy + z subject to the constraint x2 + y2 + z2 = 1. Let f ( x,y,z ) = xy + z and
g( x,y,z ) = x2 + y2 + z2 − 1. Form the Lagrangian function
L = f − λg = xy + z − λ( x2 + y2 + z2 − 1) ,
where λ is a Lagrange multiplier. Then seek unconditional stationary values of L:
Lx = y − 2 λx = 0 ,
Ly = x − 2 λy = 0 ,
Lz = 1 − 2 λz = 0 ,
Lλ = −( x2 + y2 + z2 − 1) = 0 .
As in the previous example, the ﬁrst two equations imply either x = y = 0 or y = ±x with λ = ±1
2 .
If x = y = 0 then z = ±1 (and λ = ±1
2 ) in which case f = ±1. If y = ±x and λ = ±1
2 then z = ±1
and x2 + y2 = 0, which implies x = y = 0. In either case we ﬁnd the stationary points (0 , 0 , ±1) at
which f = ±1. There is therefore a minimum of −1 at (0 , 0 , −1).
Exercise : Verify this result by parametrizing the unit sphere using spherical polar coordinates and
ﬁnding the stationary points of f ( θ,φ ).
(Note that a stationary point of a function of three variables constrained to a surface could be a saddle
point rather than a minimum or a maximum. It is not straightforward to determine the nature of the
conditional stationary point in general using the method of Lagrange multipliers.)
57
========58======
2.4.5 More than one constraint
Consider the problem of two constraints in three dimensions. We seek the stationary values of f ( x,y,z )
subject to the constraints g( x,y,z ) = 0 and h( x,y,z ) = 0. Each equation g = 0 and h = 0 deﬁnes a
surface, so we are conﬁned to the intersection of these surfaces, which is a curve. We have
d f = ( ∇ f ) ·d x ,
d g = ( ∇ g) ·d x ,
d h = ( ∇ h) ·d x .
Displacements are constrained by d g = d h = 0, which implies that d x must be perpendicular to both
∇ g and ∇ h and therefore parallel to ∇ g × ∇ h. We require d f = 0 for all such displacements. This
is satisﬁed when
∇ f = λ∇ g + µ∇ h
for some numbers λ and µ to be determined.
58
========59======
There is therefore one Lagrange multiplier for each constraint.
Equivalently, we can seek unconditional stationary values of the Lagrangian function
L( x,y,z,λ,µ ) = f ( x,y,z ) − λg ( x,y,z ) − µh ( x,y,z ) .
Example . Maximize x + y + z subject to the constraints x2 + y2 + z2 = 1 and lx + my + nz = 0.
Form the Lagrangian function
L = f − λg − µh
= x + y + z − λ( x2 + y2 + z2 − 1) − µ( lx + my + nz ) ,
where λ and µ are Lagrange multipliers. Then seek unconditional stationary values of L:
Lx = 1 − 2 λx − µl = 0 ,
Ly = 1 − 2 λy − µm = 0 ,
Lz = 1 − 2 λz − µn = 0 ,
Lλ = −( x2 + y2 + z2 − 1) = 0 ,
Lµ = −( lx + my + nz ) = 0 .
The ﬁrst three equations imply
( x,y,z ) =
1
2 λ
(1 − µl, 1 − µm, 1 − µn )
and so
f =
3 − µ( l + m + n)
2 λ
.
59
========60======
The ﬁfth equation gives
l(1 − µl ) + m (1 − µm ) + n(1 − µn ) = 0
µ =
l + m + n
l2 + m 2 + n2 .
Finally, the fourth equation gives
(1 − µl ) 2 + (1 − µm ) 2 + (1 − µn ) 2 = 4 λ2
µ2 ( l2 + m 2 + n2 ) − 2 µ( l + m + n) + 3 = 4 λ2 .
Substitute for µ:
−
( l + m + n) 2
l2 + m 2 + n2 + 3 = 4 λ2
λ = ±
1
2
[
3 −
( l + m + n) 2
l2 + m 2 + n2
]1 /2
.
Thus
f = ±
[
3 −
( l + m + n) 2
l2 + m 2 + n2
]1 /2
.
The maximum value is obtained by taking the + sign.
60
========61======
2.4.6 The Boltzmann distribution
This classic example of maximization subject to constraints comes from statistical mechanics.
Suppose we have a system consisting of a very large number of particles. Each particle occupies one
of n discrete states, labelled by the index i. There is no limit on the number of particles N i that can
occupy state i. The energy of each particle in state i is Ei.
(An example of such a system is a gas in a box. The states are discrete as a result of quantum
mechanics. The de Broglie wavelength of a particle has to equal one of a set of special values so that
it can ﬁt into the box in the form of a standing wave. There are many more such states available for
particles of shorter wavelength, and therefore higher energy.)
The total number of particles is
N =
n∑
i=1
N i.
The total energy of the system is
E =
n∑
i=1
N iEi.
The distribution of particles among states is described by the numbers ( N 1 ,N 2 ,...,N n ). A given
61
========62======
distribution ( N 1 ,N 2 ,...,N n ) can be achieved in many diﬀerent ways. The number of ways is in fact
W =
N !
N 1 !N 2 ! ··· N n !
.
This is just the number of diﬀerent ways of distributing N distinguishable objects between n boxes
such that box i contains N i objects. N ! is the number of ways of ordering all N objects, while the
division by each N i! corrects for overcounting as the ordering within each box is unimportant.
W is the number of ways to distribute particles. It characterises probability of a realisation of a given
state ( N 1 ,N 2 ,...,N n )
(The assumption of distinguishable particles leads to Maxwell–Boltzmann statistics . The fact that
particles are really indistinguishable, and fall into two categories called fermions and bosons, leads to
diﬀerent results, which are explored to some extent in Example Sheet 2, Question 21.)
It is argued in statistical mechanics that each permutation is equally probable, so the distribution
that occurs in nature is the one with the largest value of W . This can be found by maximizing the
function W ( N 1 ,N 2 ,...,N n ) with respect to the variables ( N 1 ,N 2 ,...,N n ). (When N is very large it
turns out that the maximum is extremely sharp.)
It is easier to maximize the quantity
ln W = ln( N !) −
n∑
i=1
ln( N i!) .
(The entropy is S = k ln W , where k is Boltzmann’s constant. Therefore we are ﬁnding the distribution
of maximum entropy.)
62
========63======
If the system is isolated then we should maximize ln W subject to the constraints N = ˆN = constant
and E = ˆE = constant, because the total number of particles and the total energy are ﬁxed.
We therefore seek the stationary points of the Lagrangian function
L = ln W − α( N − ˆN ) − β( E − ˆE )
= ln( N !) −
n∑
i=1
ln( N i!) − α
(
n∑
i=1
N i − ˆN
)
− β
(
n∑
i=1
N iEi − ˆE
)
,
where α and β are Lagrange multipliers.
(Although ˆN is just the ﬁxed value of N , it is important that N is here regarded as a function of the
variables ( N 1 ,N 2 ,...,N n ) while ˆN is a constant. Similarly for E and ˆE .)
In order to diﬀerentiate L, we make use of Stirling’s approximation (see last term’s lectures),
x! ≈
√
2 πx x x e −x,
valid for x ≫ 1. This implies
ln( x!) ≈ x ln x − x + 1
2 ln(2 πx )
d
d x
ln( x!) ≈ ln x +
1
2 x
≈ ln x for x ≫ 1 .
63
========64======
Applying this approximation to N ! and N i! for N ≫ 1 and N i ≫ 1, we ﬁnd
∂L
∂N i
= ln N − ln N i − α − βE i,
where we have used ∂N/∂N i = 1 to obtain the ﬁrst term. The stationary point of L therefore occurs
where
ln N − ln N i − α − βE i = 0
N i = N e −α e −βE i .
This result is the Boltzmann distribution . Physical reasoning leads to β = 1 /( kT ), where T is the
temperature. Since β > 0, states of higher energy are less occupied.
The value of α (or e −α ) can be determined from the constraint
N =
n∑
i=1
N i = N e −α
n∑
i=1
e −βE i ,
which implies
N i =
N e −βE i
∑ n
j=1 e −βE j
.
The value of β (and therefore the temperature of the system) is determined by the second constraint,
which is the total energy of the system.
64
========65======
Note that the Boltzmann distribution does not predict integer values of N i. That is because we have
treated the occupation numbers N i at several stages as large real numbers rather than integers.
The Boltzmann distribution is often discussed with degenerate states . This situation occurs when we
label the states according to their energy, but multiple states share the same energy level. The number
of states sharing the energy level Ei is the degeneracy gi. If N i now denotes the combined occupation
number of these degenerate states, then
N i = Ng i e −α e −βE i =
Ng i e −βE i
∑ n
j=1 gj e −βE j
.
This is identical to the previous result but with a diﬀerent method of counting states.
65